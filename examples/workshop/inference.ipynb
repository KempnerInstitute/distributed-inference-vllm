{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Inference for 70B Llama Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a tutorial for prompting the 70B LLama 3.1 model on the FASRC cluster. It assumes that you already have set up or have access to a server for this model created using vLLM (see instructions [here](https://github.com/KempnerInstitute/distributed-inference-vllm))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load in a few libraries necessary to requesting from the server. In particular, we use the `requests` library for making HTTP requests to the vLLM server on our node.\n",
    "\n",
    "The `MODEL_PATH` variable corresponds to the directory of the model weights - this is what vLLM uses to ID models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import asdict, dataclass\n",
    "import time\n",
    "import requests\n",
    "\n",
    "MODEL_PATH = \"/n/netscratch/kempner_dev/Everyone/models/Llama-3.1-70B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set our sampling parameters. \n",
    "\n",
    "The vLLM server allows sampling parameters similar to the OpenAI API, including adjusting temperature and whether or not log probabilities are returned with each token. We'll cover these parameters in greater detail throughout this tutorial. You can also find more details on the available sampling parameters in the [vLLM repo docs](https://docs.vllm.ai/en/v0.5.5/dev/sampling_params.html#sampling-parameters).\n",
    "\n",
    "Below, we build an `InferenceRequestParams` class which uses a subset of the available fields. These will be serialized through the `.dict()` method before being sent to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceRequestParams:\n",
    "    model: str # Should be set to MODEL_PATH\n",
    "    prompt: str\n",
    "    max_tokens: int\n",
    "    min_tokens: int = 0\n",
    "    temperature: float = 1.0\n",
    "    frequency_penalty: float = 0.0\n",
    "    top_k: int = -1\n",
    "    logprobs: int | None = None\n",
    "    prompt_logprobs: int | None = None\n",
    "\n",
    "    def dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "request_params = InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", 10, temperature=1.0, logprobs=0, prompt_logprobs=0)\n",
    "request_params.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Inference for a single prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to prompt our LLM model! \n",
    "\n",
    "**The details of how we do this:** we can send a request to the server via an HTTP POST request to the `/v1/completions` endpoint. This can be done via the `requests.post` function. We use the `json` argument to send a JSON payload with our sampling parameters to our server. The response will be returned as a JSON, which will contain the text completion as well as some metadata about the request.\n",
    "\n",
    "1) **Find the IP of the node hosting the server.** You can find this as the `head node ip` detailed in the output logs of the slurm job. Replace the `FIXME` in the code below with a string containing the ip address you find in the logs. If you've already opened up VSCode remote development on the head compute node, you can use `localhost` for the IP address. \n",
    "\n",
    "2) **Run the code and take a look at the outputs.** Do you understand what each output represents? \n",
    "\n",
    "3) **Try changing the `max_tokens` and `min_tokens` value in the code below** and rerunning the code to see what outputs you get. \n",
    "\n",
    "4) **Try changing the `temperature` value in the code below**. Run the code a couple times for each temperature parameter you try. What do you notice about the output text when temperature is 0 vs higher? \n",
    "\n",
    "5) **Try changing the `frequency_penalty` value in the code below**. It can take any value between [-2, 2]. Can you figure out what this parameter does?\n",
    "\n",
    "6) **Try changing the `top_k` value in the code below**. Use temperature=1. How do the outputs change?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_ADDRESS='10.31.147.3'\n",
    "def send_request(request_params: InferenceRequestParams):\n",
    "    response = requests.post(f'http://{IP_ADDRESS}:8000/v1/completions', json = request_params.dict())\n",
    "    return response.json()\n",
    "\n",
    "send_request(InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", max_tokens=10, min_tokens=0, temperature=0.0, frequency_penalty=0.0, top_k=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Investigating log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `InferenceRequestParams` class, you can change the `logprob` and `prompt_logprob` fields in order to extract the log probabilities of the tokens in the completion and the prompt, respectively.\n",
    "\n",
    "When set to `None` (the default), the log probabilities are not returned by the server.  When set to non-negative integer `k`, the server will return the top `k` highest log probabilities at each token generation step, along with the log probability of generated token if it is not in the top `k`. Note that if `k=0`, then the server will just return the log probabilities of the generated tokens.\n",
    "\n",
    "In the code below, we return the output as the variable `response`, which is a dictionary. We've already returned just the relevant element of the dictionary using the key `choices`. \n",
    "\n",
    "Note that `response['choices']` is a list of length one. To access the logprobs values, you need to use `response['choices'][0]['logprobs']`. This formatting might seem odd — why is it a list with a single element? The reason is that certain sampling parameter choices can return multiple output sequences, making `response['choices']` a list with multiple elements. However, we won’t cover those parameters in this tutorial.\n",
    "\n",
    "**Try re-running the following code with different values of `logprobs` and `prompt_logprobs` and inspect the outputs.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = send_request(InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", max_tokens=10, temperature=1.0, logprobs=None, prompt_logprobs=None))\n",
    "\n",
    "response['choices']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Inference for multiple prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we would like to run multiple prompts at once (in parallel), instead of one-at-a-time? \n",
    "\n",
    "To process multiple prompts, we can make use of Python's multithreading to send multiple requests to the server. We put the prompts on the queue and create `NUM_THREADS` thread workers to process the queue. Each worker will independently pull prompts from the queue and send the corresponding request to the server. \n",
    "\n",
    "You may be familiar with the idea of batching prompts together, which is what we do when training LLMs. Compared to batching the prompts and processing each batch one by one, this queue method achieves better performance. This is because when a prompt finishes quickly, the corresponding worker will automatically pull the next prompt from the queue. In comparison, in batch processing, you need to wait for the slowest prompt within a batch to finish before the next batch gets processed, even if most of the other prompts within that batch finished quickly.\n",
    "\n",
    "1) Given the multithreading description, how would you expect the execution time with four prompts and 4 threads to compare with 3 prompts and 4 threads? How about compared to 5 prompts and 4 threads?\n",
    "2) Try running the following code below. Add or remove prompts and change the value of `NUM_THREADS`. Look at the execution time. Does it match your expectations?\n",
    "3) Try changing max_tokens and look at the effect on execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "prompts = [\"San Francisco is a \", \"Boston is a \", \"Chicago is a \", \"NYC is a\"]\n",
    "params = [InferenceRequestParams(MODEL_PATH, prompt, max_tokens=200, temperature=0.0) for prompt in prompts]\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREADS) as pool:\n",
    "    responses = pool.map(send_request, params)\n",
    "print(f\"Total time: {time.time() - start_time}\")\n",
    "[response['choices'][0]['text'] for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using the output logs to analyze performance\n",
    "\n",
    "The output logs for the SLURM job running your server will show logs when requests are received, as well as statistics on the number of tokens being processed per second and KV cache usage. This can be helpful for debugging and performance analysis.\n",
    "\n",
    "Run the following code and look at the output log file. Watch as the requests are received. You should see that the KV cache memory usage grows over time as more tokens are generated with each request. The usage should also drop as each request finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "params = InferenceRequestParams(MODEL_PATH, \"Give me a list of 100 history questions related to American history. 1. How did \", 5000, temperature=0.0)\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREADS) as pool:\n",
    "    for i in range(NUM_THREADS):\n",
    "        pool.submit(send_request, params)\n",
    "        time.sleep(25)\n",
    "f\"Total time: {time.time() - start_time}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
