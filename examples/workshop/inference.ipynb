{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a few libraries for requesting from the server. In particular, we use the `requests` library for making HTTP requests to the vLLM server on our node.\n",
    "\n",
    "The `MODEL_PATH` variable is declared to match the directory of the model weights, since this is what vLLM uses to ID models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import asdict, dataclass\n",
    "import time\n",
    "import requests\n",
    "\n",
    "MODEL_PATH = \"/n/netscratch/kempner_dev/Everyone/models/Llama-3.1-70B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vLLM server allows sampling parameterss similar to the OpenAI API. These sampling parameters allow one to change things like temperature and top_k, as well as control whether or not log probabilities are returned with each token. You can find more details on the available sampling parameters in the (vLLM repo)[https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py#L87].\n",
    "\n",
    "In the `InferenceRequestParams` class below, we use a subset of the available fields, which will be serialized through the `.dict()` method before being sent to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceRequestParams:\n",
    "    model: str # Should be set to MODEL_PATH\n",
    "    prompt: str\n",
    "    max_tokens: int\n",
    "    best_of: int = 1\n",
    "    n: int = 1\n",
    "    temperature: float = 1.0\n",
    "    frequency_penalty: float = 0.0\n",
    "    top_k: int = -1\n",
    "    logprobs: int | None = None\n",
    "    prompt_logprobs: int | None = None\n",
    "\n",
    "    def dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "request_params = InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", 10, n = 2, best_of = 4, temperature=1.0, logprobs=0, prompt_logprobs=0)\n",
    "request_params.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send a request to the server via an HTTP POST request to the `localhost:8000/v1/completions` endpoint. This can be done via the `requests.post` function. We use the `json` argument to send a JSON payload with our sampling parameters to our server. The response will be returned as a JSON, which will contain the text completion as well as some metadata about the request. \n",
    "\n",
    "Try changing the `temperature` value in the code below and rerunning the code to see what outputs you get. You'll notice that `temperature=0.0` always produces the same value, but higher `temperature` values become more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(request_params: InferenceRequestParams):\n",
    "    response = requests.post('http://localhost:8000/v1/completions', json = request_params.dict())\n",
    "    return response.json()\n",
    "\n",
    "send_request(InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", 10, temperature=FIXME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also `logprob` and `prompt_logprob` fields for extracting the log probabilities of the tokens in the completion and the prompt, respectively. When set to `None` (the default), the log probabilities are not returned by the server, but when set to a non-negative integer `k`, the server will return the top `k` highest log probabilities at each token generation step, along with the log probability of generated token if it is not in the top `k`. Note that if `k=0`, then the server will just return the log probabilities of the generated tokens.\n",
    "\n",
    "Try rerunning the following code with different values of `logprobs` and `prompt_logprobs` and see what outputs you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_request(InferenceRequestParams(MODEL_PATH, \"San Francisco is a \", 10, temperature=1.0, logprobs=FIXME, prompt_logprobs=FIXME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process multiple prompts, we make use of Python's multithreading to send multiple requests to the server. We put the prompts on the queue and create `NUM_THREADS` thread workers to process the queue. Each worker will independently pull prompts from the queue and send the corresponding request to the server. Compared to batching the prompts and processing each batch one by one, this queue method achieves better performance, because when a prompt finishes quickly, the corresponding worker will automatically pull the next prompt from the queue. In comparison, a batch needs to wait for the slowest prompt to finish before the next batch gets processed, even if most of the other prompts finished quickly.\n",
    "\n",
    "Try running the following code below. Feel free to add or remove prompts and change ht4e value of `NUM_THREADS`. See how the execution time changes based on the length of the `prompts` list and `NUM_THREADS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "prompts = [\"San Francisco is a \", \"Boston is a \", \"Chicago is a \", \"New York is a \"]\n",
    "params = [InferenceRequestParams(MODEL_PATH, prompt, 2000, 0.0) for prompt in prompts]\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREADS) as pool:\n",
    "    responses = pool.map(send_request, params)\n",
    "print(f\"Total time: {time.time() - start_time}\")\n",
    "[response['choices'][0]['text'] for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the output logs for the SLURM job running your server. It will show logs for when requests are received, as well as statistics on the number of tokens being processed per second and KV cache usage. This can be helpful for debugging and performance analysis.\n",
    "\n",
    "Run the following code and look at the log file. Watch as the requests are received and how the KV cache memory usage grows over time as more tokens are generated with each request. The usage should also drop as each request finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 256\n",
    "prompts = [\"San Francisco is a \"]*500\n",
    "params = [InferenceRequestParams(MODEL_PATH, prompt, 10000, 1.0) for prompt in prompts]\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREADS) as pool:\n",
    "    responses = pool.map(send_request, params)\n",
    "f\"Total time: {time.time() - start_time}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
